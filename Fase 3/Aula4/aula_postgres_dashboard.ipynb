{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Gerar Diagrama Entidade-Relacionamento (DER) com dbdiagram.io\n",
    "\n",
    "### Passo a passo:\n",
    "1. Acesse o site: [https://dbdiagram.io](https://dbdiagram.io)\n",
    "2. Clique em **\"New Diagram\"** ou fa√ßa login se necess√°rio.\n",
    "3. No editor da esquerda, cole o seguinte c√≥digo em formato **DBML**.\n",
    "4. O diagrama ser√° gerado automaticamente na √°rea √† direita.\n",
    "5. Voc√™ pode exportar como imagem ou compartilhar com sua turma.\n",
    "\n",
    "üëá **C√≥digo DBML para colar no dbdiagram.io:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Modelo Relacional em DBML (dbdiagram.io)\n",
    "\n",
    "```dbml\n",
    "Table Clientes {\n",
    "  id_cliente int [pk]\n",
    "  nome varchar\n",
    "  email varchar\n",
    "}\n",
    "\n",
    "Table TipoProdutos {\n",
    "  id_tipo int [pk]\n",
    "  descricao varchar\n",
    "}\n",
    "\n",
    "Table Produtos {\n",
    "  id_produto int [pk]\n",
    "  nome varchar\n",
    "  preco decimal\n",
    "  id_tipo int [ref: > TipoProdutos.id_tipo, unique]\n",
    "}\n",
    "\n",
    "Table Pedidos {\n",
    "  id_pedido int [pk]\n",
    "  data_pedido date\n",
    "  id_cliente int [ref: > Clientes.id_cliente]\n",
    "}\n",
    "\n",
    "Table ItensPedido {\n",
    "  id_pedido int [ref: > Pedidos.id_pedido]\n",
    "  id_produto int [ref: > Produtos.id_produto]\n",
    "  quantidade int\n",
    "  indexes {\n",
    "    (id_pedido, id_produto) [pk]\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Diagrama Entidade-Relacionamento (DER)\n",
    "\n",
    "Abaixo est√° o diagrama visual gerado com base nas tabelas definidas:\n",
    "\n",
    "![DER Relacional](der-relacional.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Modelo Dimensional em DBML ‚Äì Diagrama DW (dimens√µes e fato)\n",
    "\n",
    "```dbml\n",
    "Table dim_cliente {\n",
    "  id_cliente int [pk]\n",
    "  nome varchar\n",
    "  idade int\n",
    "  cidade varchar\n",
    "}\n",
    "\n",
    "Table dim_produto {\n",
    "  id_produto int [pk]\n",
    "  nome_produto varchar\n",
    "  categoria varchar\n",
    "  preco decimal\n",
    "}\n",
    "\n",
    "Table fato_pedidos {\n",
    "  id_pedido int [pk]\n",
    "  id_cliente int [ref: > dim_cliente.id_cliente]\n",
    "  id_produto int [ref: > dim_produto.id_produto]\n",
    "  data_pedido date\n",
    "  quantidade int\n",
    "  valor_total decimal\n",
    "}\n",
    "\n",
    "Table dim_data {\n",
    "  id_data int [pk]\n",
    "  data date\n",
    "  ano int\n",
    "  mes int\n",
    "  dia int\n",
    "  dia_semana varchar\n",
    "  nome_mes varchar\n",
    "}\n",
    "\n",
    "Table dim_regiao {\n",
    "  id_regiao int [pk]\n",
    "  nome_regiao varchar\n",
    "  estado varchar\n",
    "  cidade varchar\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Diagrama Entidade-Relacionamento (DER)\n",
    "\n",
    "Abaixo est√° o diagrama visual gerado com base nas tabelas definidas:\n",
    "\n",
    "![DER Relacional](der-dw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9e66e",
   "metadata": {},
   "source": [
    "# üìò Passo a Passo: Configura√ß√£o do PostgreSQL na AWS RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1c0eb",
   "metadata": {},
   "source": [
    "## 1. Criar inst√¢ncia RDS com PostgreSQL (Free Tier)\n",
    "\n",
    "1. Acesse o console AWS ‚Üí [https://console.aws.amazon.com/rds/](https://console.aws.amazon.com/rds/)\n",
    "2. Clique em **Criar banco de dados**\n",
    "3. Selecione:\n",
    "   - **Tipo de banco:** PostgreSQL\n",
    "   - **Modelo de uso:** Free Tier\n",
    "   - **Vers√£o:** PostgreSQL 15 (ou mais recente)\n",
    "   - **Identificador da inst√¢ncia:** `bd-relacional`\n",
    "   - **Usu√°rio:** `postgres`\n",
    "   - **Senha:** crie uma senha segura\n",
    "4. Tipo de inst√¢ncia: `db.t3.micro`\n",
    "5. Armazenamento: 20 GB (SSD General Purpose)\n",
    "6. **Acesso p√∫blico:** Habilitado (Sim)\n",
    "7. Clique em **Criar banco de dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0dab4",
   "metadata": {},
   "source": [
    "## 2. Liberar o IP na VPC / Grupo de Seguran√ßa (Security Group)\n",
    "\n",
    "1. V√° para **EC2 > Grupos de Seguran√ßa**\n",
    "2. Encontre o grupo associado √† inst√¢ncia RDS\n",
    "3. Clique em **Editar regras de entrada**\n",
    "4. Adicione uma nova regra:\n",
    "   - Tipo: `PostgreSQL`\n",
    "   - Porta: `5432`\n",
    "   - Origem: `Seu IP` (ou `0.0.0.0/0` temporariamente para teste ‚Äì cuidado com isso em produ√ß√£o)\n",
    "5. Salve as altera√ß√µes.\n",
    "\n",
    "‚úÖ Agora o acesso externo ao banco estar√° liberado para seu IP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94c38f",
   "metadata": {},
   "source": [
    "## 3. Copie o Endpoint da RDS\n",
    "\n",
    "1. Volte ao RDS > Banco de dados > `bd-relacional`\n",
    "2. Copie o valor do campo **Endpoint** (algo como `bd-relacional.xxxxxx.us-east-1.rds.amazonaws.com`)\n",
    "3. Use esse endpoint no notebook para se conectar com o PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056c4e8",
   "metadata": {},
   "source": [
    "# Aula Pr√°tica ‚Äì Banco Relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalar bibliotecas necessarias\n",
    "\n",
    "%pip install sqlalchemy psycopg2-binary pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando vers√£o bin√°ria para evitar erros de compila√ß√£o\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bacef",
   "metadata": {},
   "source": [
    "## Conectando ao PostgreSQL (RDS ou local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93193ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitua pelos seus dados\n",
    "usuario = \"postgres\"\n",
    "senha = \"Fiap#2025\"\n",
    "host = \"postgres-db.cu4mvwwdzs1u.us-east-1.rds.amazonaws.com\"\n",
    "porta = 5432\n",
    "banco = \"db_relacional\"\n",
    "\n",
    "# Cria a engine de conex√£o\n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario}:{senha}@{host}:{porta}/{banco}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9834b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection(engine):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # Testa a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Lista as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"  -\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b94b3",
   "metadata": {},
   "source": [
    "## Cria√ß√£o do Modelo Relacional (Clientes, Produtos, Tipos, Pedidos, Itens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de comandos individuais\n",
    "ddl_commands = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tipos_produto (\n",
    "      id_tipo SERIAL PRIMARY KEY,\n",
    "      nome_tipo VARCHAR(50) NOT NULL\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS produtos (\n",
    "      id_produto SERIAL PRIMARY KEY,\n",
    "      nome_produto VARCHAR(100) NOT NULL,\n",
    "      preco DECIMAL(10,2) NOT NULL,\n",
    "      id_tipo INT REFERENCES tipos_produto(id_tipo)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS clientes (\n",
    "      id_cliente SERIAL PRIMARY KEY,\n",
    "      nome VARCHAR(100) NOT NULL,\n",
    "      email VARCHAR(100),\n",
    "      telefone VARCHAR(20), \n",
    "      cidade VARCHAR(100) NOT NULL, \n",
    "      estado VARCHAR(2) NOT NULL\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pedidos (\n",
    "      id_pedido SERIAL PRIMARY KEY,\n",
    "      data_pedido DATE NOT NULL,\n",
    "      status VARCHAR(20) NOT NULL,\n",
    "      id_cliente INT NOT NULL REFERENCES clientes(id_cliente)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS itens_pedido (\n",
    "    id_item SERIAL PRIMARY KEY,\n",
    "    id_pedido INT NOT NULL,\n",
    "    id_produto INT NOT NULL,\n",
    "    quantidade INT NOT NULL,\n",
    "    preco_unitario DECIMAL(10,2) NOT NULL,\n",
    "    CONSTRAINT fk_pedido FOREIGN KEY (id_pedido) REFERENCES pedidos(id_pedido) ON DELETE CASCADE,\n",
    "    CONSTRAINT fk_produto FOREIGN KEY (id_produto) REFERENCES produtos(id_produto) ON DELETE CASCADE\n",
    "    );\n",
    "\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Executar comandos um a um\n",
    "with engine.begin() as conn:\n",
    "    for cmd in ddl_commands:\n",
    "        conn.execute(text(cmd))\n",
    "\n",
    "print(\"‚úÖ Tabelas criadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\"))\n",
    "    for row in result:\n",
    "        print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 1. Verificar Chaves Prim√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pks = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "    kcu.table_schema,\n",
    "    kcu.table_name,\n",
    "    kcu.column_name,\n",
    "    tc.constraint_name\n",
    "FROM information_schema.table_constraints tc\n",
    "JOIN information_schema.key_column_usage kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "WHERE tc.constraint_type = 'PRIMARY KEY'\n",
    "  AND kcu.table_schema = 'public';\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_pks.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó 2. Verificar Chaves Estrangeiras e Relacionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fks = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "    tc.table_name AS tabela_origem,\n",
    "    kcu.column_name AS coluna_origem,\n",
    "    ccu.table_name AS tabela_referenciada,\n",
    "    ccu.column_name AS coluna_referenciada\n",
    "FROM information_schema.table_constraints AS tc\n",
    "JOIN information_schema.key_column_usage AS kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "JOIN information_schema.constraint_column_usage AS ccu\n",
    "  ON ccu.constraint_name = tc.constraint_name\n",
    "WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "  AND tc.table_schema = 'public';\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_fks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Consulta combinada (vis√£o geral dos relacionamentos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relacionamentos = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "  tc.constraint_name,\n",
    "  tc.table_name AS origem,\n",
    "  kcu.column_name AS coluna_origem,\n",
    "  ccu.table_name AS destino,\n",
    "  ccu.column_name AS coluna_destino\n",
    "FROM information_schema.table_constraints AS tc\n",
    "JOIN information_schema.key_column_usage AS kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "JOIN information_schema.constraint_column_usage AS ccu\n",
    "  ON ccu.constraint_name = tc.constraint_name\n",
    "WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "ORDER BY origem;\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_relacionamentos.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0545039",
   "metadata": {},
   "source": [
    "## Inser√ß√£o de dados simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a47751",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_script = \"\"\"\n",
    "\n",
    "-- Tipos de produto\n",
    "INSERT INTO TipoProdutos (descricao) VALUES \n",
    "('Medicamento'), ('Suplemento') \n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- Produtos\n",
    "INSERT INTO Produtos (nome, preco, id_tipo) VALUES \n",
    "('Paracetamol', 19.99, 1),\n",
    "('Vitamina C', 29.99, 2) \n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- Clientes\n",
    "INSERT INTO Clientes (nome, email) VALUES \n",
    "('Carlos Silva', 'carlos@email.com'),\n",
    "('Ana Souza', 'ana@email.com') \n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- Pedidos\n",
    "INSERT INTO Pedidos (data_pedido, id_cliente) VALUES \n",
    "('2025-06-01', 1),\n",
    "('2025-06-02', 2) \n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- Itens de pedido\n",
    "INSERT INTO ItensPedido (id_pedido, id_produto, quantidade) VALUES \n",
    "(1, 1, 2),\n",
    "(1, 2, 1),\n",
    "(2, 2, 3)\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "# Executar os inserts via SQLAlchemy\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(insert_script))\n",
    "    print(\"‚úÖ Dados inseridos com sucesso (com prote√ß√£o contra duplicatas).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lista de arquivos a serem executados (ordem importa por causa das FKs)\n",
    "sql_files = [\n",
    "    \"sql/aula4/clientes.sql\",\n",
    "    \"sql/aula4/tipos_produto.sql\",\n",
    "    \"sql/aula4/produtos.sql\",\n",
    "    \"sql/aula4/pedidos.sql\",\n",
    "    \"sql/aula4/itens_pedido.sql\"\n",
    "]\n",
    "\n",
    "# 3. Executar cada arquivo SQL\n",
    "with engine.begin() as conn:\n",
    "    for file_path in sql_files:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            sql_content = file.read()\n",
    "            conn.execute(text(sql_content))\n",
    "            print(f\"‚úÖ Executado: {file_path.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131d760",
   "metadata": {},
   "source": [
    "## Consulta com JOIN para an√°lise de vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "  c.nome AS cliente,\n",
    "  p.data_pedido,\n",
    "  pr.nome AS produto,\n",
    "  pr.preco,\n",
    "  t.descricao AS tipo_produto,\n",
    "  ip.quantidade,\n",
    "  (pr.preco * ip.quantidade) AS total_venda\n",
    "FROM ItensPedido ip\n",
    "JOIN Pedidos p ON ip.id_pedido = p.id_pedido\n",
    "JOIN Clientes c ON p.id_cliente = c.id_cliente\n",
    "JOIN Produtos pr ON ip.id_produto = pr.id_produto\n",
    "JOIN TipoProdutos t ON pr.id_tipo = t.id_tipo;\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5dfa6",
   "metadata": {},
   "source": [
    "## Gera√ß√£o de Dashboard com Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df, x=\"cliente\", y=\"total_venda\", color=\"produto\", \n",
    "             title=\"Total de Vendas por Cliente e Produto\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Script para deletar todas as tabelas (com SQLAlchemy)\n",
    "\n",
    "Script SQL completo para deletar todas as tabelas na ordem correta, respeitando os relacionamentos (constraints).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Observa√ß√µes importantes:\n",
    "\n",
    "  - O `CASCADE` garante que todas as depend√™ncias (FKs) sejam eliminadas junto com a tabela.**\n",
    "  - Para deletar o banco de dados, voc√™ precisa:**\n",
    "  - Estar conectado a outro banco, como `postgres`.\n",
    "  - Ter permiss√µes de superusu√°rio ou ser o dono do banco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_script = \"\"\"\n",
    "DROP TABLE IF EXISTS itens_pedido CASCADE;\n",
    "DROP TABLE IF EXISTS pedidos CASCADE;\n",
    "DROP TABLE IF EXISTS produtos CASCADE;\n",
    "DROP TABLE IF EXISTS clientes CASCADE;\n",
    "DROP TABLE IF EXISTS tipos_produto CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "# Forma correta de usar raw_connection\n",
    "conn = engine.raw_connection()\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(drop_script)\n",
    "    conn.commit()\n",
    "    print(\"üóëÔ∏è Todas as tabelas foram deletadas com sucesso.\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd3174",
   "metadata": {},
   "source": [
    "## Encerrando conex√£o do Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6052c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula Pr√°tica ‚Äì Data Warehouse e Data Lake na AWS\n",
    "\n",
    "![DW](arq-dw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Modelo Dimensional (Star Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß± Cria√ß√£o das Tabelas Dimens√£o e Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "ddl_dim_fato = \"\"\"\n",
    "-- Cria√ß√£o das tabelas do modelo dimensional\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dim_cliente (\n",
    "  id_cliente INT PRIMARY KEY,\n",
    "  nome VARCHAR(100),\n",
    "  idade INT,\n",
    "  cidade VARCHAR(100)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dim_produto (\n",
    "  id_produto INT PRIMARY KEY,\n",
    "  nome_produto VARCHAR(100),\n",
    "  categoria VARCHAR(50),\n",
    "  preco DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS fato_pedidos (\n",
    "  id_pedido INT PRIMARY KEY,\n",
    "  id_cliente INT REFERENCES dim_cliente(id_cliente),\n",
    "  id_produto INT REFERENCES dim_produto(id_produto),\n",
    "  data_pedido DATE,\n",
    "  quantidade INT,\n",
    "  valor_total DECIMAL(10,2)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Executar cria√ß√£o e inser√ß√£o\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(ddl_dim_fato))\n",
    "    print(\"‚úÖ Tabelas dimensionais criadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Inser√ß√£o na tabela dimens√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserts_dim = \"\"\"\n",
    "-- Inser√ß√£o nas tabelas de dimens√£o\n",
    "\n",
    "INSERT INTO dim_cliente VALUES\n",
    "(1, 'Carlos Silva', 35, 'S√£o Paulo'),\n",
    "(2, 'Ana Souza', 28, 'Rio de Janeiro'),\n",
    "(3, 'Marcos Lima', 42, 'Belo Horizonte')\n",
    "ON CONFLICT (id_cliente) DO NOTHING;\n",
    "\n",
    "INSERT INTO dim_produto VALUES\n",
    "(1001, 'Vitamina C', 'Suplementos', 29.99),\n",
    "(1002, 'Paracetamol', 'Medicamentos', 19.99),\n",
    "(1003, 'Protetor Solar', 'Beleza', 39.99)\n",
    "ON CONFLICT (id_produto) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "# Executar cria√ß√£o e inser√ß√£o\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(inserts_dim))\n",
    "    print(\"‚úÖ Tabelas dimensionais populadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Obs. importante: o uso de ON CONFLICT (id_cliente) DO NOTHING exige que os campos estejam marcados como PRIMARY KEY (o que j√° est√°), garantindo que a inser√ß√£o seja idempotente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Inser√ß√£o na tabela fato_pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_fato = \"\"\"\n",
    "INSERT INTO fato_pedidos VALUES\n",
    "(101, 1, 1001, '2025-02-10', 2, 59.98),\n",
    "(102, 2, 1002, '2025-02-11', 1, 19.99),\n",
    "(103, 3, 1003, '2025-02-12', 3, 119.97)\n",
    "ON CONFLICT (id_pedido) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(insert_fato))\n",
    "    print(\"‚úÖ Fatos inseridos com sucesso na tabela fato_pedidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Notas:\n",
    "\n",
    "O ON CONFLICT (id_pedido) DO NOTHING garante que o script pode ser executado mais de uma vez sem erro, evitando duplicidade.\n",
    "\n",
    "Certifique-se de que a tabela fato_pedidos j√° existe e est√° corretamente relacionada √†s tabelas dim_cliente e dim_produto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Observa√ß√£o sobre ItensPedido no Modelo Dimensional\n",
    "\n",
    "No modelo dimensional (**Data Warehouse**), n√£o √© obrigat√≥rio manter tabelas como `ItensPedido` ‚Äî que s√£o comuns no modelo relacional ‚Äî porque as **tabelas fato j√° s√£o desenhadas para representar os eventos de neg√≥cio com a granularidade desejada**.\n",
    "\n",
    "Ou seja, no DW √© comum que a tabela fato (`fato_pedidos`) j√° registre diretamente cada produto comprado em um pedido, eliminando a necessidade de uma tabela intermedi√°ria como no modelo relacional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ O que muda no DW (Data Warehouse)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ No modelo relacional (OLTP):\n",
    "\n",
    "Voc√™ tem:\n",
    "\n",
    "- `Pedidos`: representa o pedido (cabe√ßalho)\n",
    "- `ItensPedido`: representa cada produto do pedido (detalhe)\n",
    "- `Produtos`: cat√°logo dos itens\n",
    "\n",
    "üîÅ Relacionamento **N:M** ‚Üí precisa da tabela intermedi√°ria `ItensPedido`\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ No modelo dimensional (DW/OLAP):\n",
    "\n",
    "Voc√™ quer analisar **fatos (eventos)** como:\n",
    "\n",
    "- Vendas realizadas\n",
    "- Produtos vendidos\n",
    "- Clientes que compraram\n",
    "- Datas de pedidos\n",
    "\n",
    "‚û°Ô∏è Nesse caso, voc√™ cria uma **tabela fato com granularidade de item**, ou seja:  \n",
    "**cada linha da `fato_pedidos` representa uma combina√ß√£o de produto + pedido + cliente.**\n",
    "\n",
    "‚úîÔ∏è Ou seja, a tabela `fato_pedidos` **j√° incorpora o papel da `ItensPedido`**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Comparando OLTP vs DW\n",
    "\n",
    "| Modelo Relacional (OLTP)    | Modelo Dimensional (DW)                   |\n",
    "|-----------------------------|-------------------------------------------|\n",
    "| `Pedidos` + `ItensPedido`   | `fato_pedidos` com granularidade de item  |\n",
    "| `Produtos`, `Clientes`      | `dim_produto`, `dim_cliente`              |\n",
    "| Normaliza√ß√£o (3FN)          | Desnormaliza√ß√£o controlada                |\n",
    "| Evita redund√¢ncia           | Otimiza performance de leitura            |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Conclus√£o:\n",
    "\n",
    "Voc√™ **n√£o precisa da `ItensPedido` no DW**.\n",
    "\n",
    "A tabela `fato_pedidos`, com colunas como `id_cliente`, `id_produto`, `data_pedido`, `quantidade` e `valor_total`, **j√° representa cada item comprado de forma anal√≠tica e otimizada para consulta.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíß Introdu√ß√£o ao Data Lake\n",
    "\n",
    "O **Data Lake** √© uma arquitetura de armazenamento de dados que permite armazenar grandes volumes de dados em seus formatos brutos e variados ‚Äî estruturados, semi-estruturados e n√£o estruturados ‚Äî de forma centralizada, escal√°vel e econ√¥mica.\n",
    "\n",
    "Diferente de um Data Warehouse, que exige esquemas bem definidos no momento da ingest√£o (schema-on-write), o Data Lake adota o paradigma **schema-on-read**, permitindo maior flexibilidade para cientistas de dados, engenheiros e analistas explorarem os dados conforme suas necessidades.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Caracter√≠sticas principais:\n",
    "\n",
    "- Armazena dados em grande escala, com baixo custo (ex: Amazon S3, Azure Data Lake)\n",
    "- Suporta dados brutos: JSON, CSV, imagens, v√≠deos, logs, IoT, entre outros\n",
    "- Possui camadas l√≥gicas como: **Raw**, **Cleansed**, **Curated**\n",
    "- Permite m√∫ltiplos consumidores de dados (BI, IA, machine learning, APIs)\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Vis√£o Geral da Arquitetura de um Data Lake\n",
    "\n",
    "![Arquitetura de Data Lake](ard-dl.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Benef√≠cios do Data Lake\n",
    "\n",
    "- Centraliza√ß√£o de dados corporativos\n",
    "- Flexibilidade para armazenar qualquer tipo de dado\n",
    "- Facilidade de integra√ß√£o com ferramentas anal√≠ticas (Spark, Athena, Glue, Databricks)\n",
    "- Apoio √† cultura Data-Driven e democratiza√ß√£o do acesso aos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Formatos de Arquivo em Data Lake e Lakehouse\n",
    "\n",
    "Em ambientes de dados modernos, o **formato de armazenamento** impacta diretamente o desempenho, compress√£o, custo e usabilidade dos dados. Abaixo est√£o os formatos mais comuns usados no contexto de Data Lake e Lakehouse:\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ CSV (Comma-Separated Values)\n",
    "\n",
    "- **Tipo:** Texto plano\n",
    "- **Caracter√≠sticas:**\n",
    "  - Leitura universal (compat√≠vel com Excel, pandas, SQL, etc)\n",
    "  - F√°cil de inspecionar visualmente\n",
    "- **Desvantagens:**\n",
    "  - Sem compress√£o nativa\n",
    "  - N√£o suporta tipos complexos (array, struct)\n",
    "  - Consome mais espa√ßo\n",
    "- **Recomendado para:** ingest√£o inicial, pequenos conjuntos de dados, exporta√ß√µes simples\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Parquet\n",
    "\n",
    "- **Tipo:** Colunar, bin√°rio\n",
    "- **Caracter√≠sticas:**\n",
    "  - Compacta√ß√£o eficiente\n",
    "  - Leitura seletiva de colunas (ideal para consultas anal√≠ticas)\n",
    "  - Suporte a esquema e tipos complexos\n",
    "- **Desvantagens:**\n",
    "  - N√£o √© leg√≠vel diretamente por humanos\n",
    "- **Recomendado para:** Data Lake anal√≠tico, tabelas intermedi√°rias no Lakehouse, grandes volumes de dados\n",
    "\n",
    "---\n",
    "\n",
    "### üìó ORC (Optimized Row Columnar)\n",
    "\n",
    "- **Tipo:** Colunar, bin√°rio (muito usado com Hive)\n",
    "- **Caracter√≠sticas:**\n",
    "  - Alta compacta√ß√£o e performance em queries\n",
    "  - Metadados embutidos no arquivo\n",
    "- **Desvantagens:**\n",
    "  - Otimizado principalmente para o ecossistema Hadoop (Hive, Impala)\n",
    "- **Recomendado para:** ambientes Hadoop, grandes volumes em clusters Hive\n",
    "\n",
    "---\n",
    "\n",
    "### üí† Delta (Delta Lake Format)\n",
    "\n",
    "- **Tipo:** Extens√£o do Parquet com controle de transa√ß√µes (ACID)\n",
    "- **Caracter√≠sticas:**\n",
    "  - Hist√≥rico de vers√µes dos dados\n",
    "  - Suporte a atualiza√ß√µes, deletes e merge (tipo banco de dados)\n",
    "  - Compat√≠vel com Apache Spark\n",
    "- **Desvantagens:**\n",
    "  - Requer engine com suporte a Delta (ex: Spark, Databricks, EMR com Delta)\n",
    "- **Recomendado para:** Lakehouse, pipelines com camadas Bronze/Silver/Gold, governan√ßa e qualidade de dados\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Resumo: Quando usar cada formato\n",
    "\n",
    "| Formato  | Leitura Humana | Compacta√ß√£o | Tipagem | Transa√ß√µes ACID | Quando Usar                      |\n",
    "|----------|----------------|-------------|---------|------------------|----------------------------------|\n",
    "| CSV      | ‚úÖ              | ‚ùå          | ‚ùå      | ‚ùå               | Simples, exporta√ß√µes manuais     |\n",
    "| Parquet  | ‚ùå              | ‚úÖ          | ‚úÖ      | ‚ùå               | Consultas anal√≠ticas, Data Lake  |\n",
    "| ORC      | ‚ùå              | ‚úÖ          | ‚úÖ      | ‚ùå               | Hadoop/Hive/Impala               |\n",
    "| Delta    | ‚ùå              | ‚úÖ          | ‚úÖ      | ‚úÖ               | Lakehouse, atualiza√ß√µes e merges |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Formatos de Arquivo em Data Lake e Lakehouse\n",
    "\n",
    "---\n",
    "\n",
    "### üß† O que √© \"Tipagem\"?\n",
    "\n",
    "**Tipagem** refere-se √† capacidade do formato de arquivo armazenar **tipos de dados estruturados** de forma expl√≠cita, como:\n",
    "\n",
    "- `string`, `integer`, `float`, `boolean`\n",
    "- e at√© estruturas complexas como `array`, `map`, `struct`\n",
    "\n",
    "Formatos com **tipagem forte** permitem leitura otimizada, valida√ß√£o de schema e compatibilidade com engines anal√≠ticas como Spark, Hive, Athena, BigQuery.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Comparativo dos principais formatos\n",
    "\n",
    "| Formato  | Leitura Humana | Compacta√ß√£o | Tipagem | Suporta ACID | Benef√≠cios Principais                                           | Contras / Limita√ß√µes                                     |\n",
    "|----------|----------------|-------------|---------|--------------|------------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **CSV**  | ‚úÖ Sim          | ‚ùå N√£o       | ‚ùå N√£o   | ‚ùå N√£o        | Leitura universal, f√°cil de gerar, visualmente simples          | Sem compress√£o, sem schema, f√°cil de quebrar com dados sujos |\n",
    "| **Parquet** | ‚ùå N√£o       | ‚úÖ Alta      | ‚úÖ Sim   | ‚ùå N√£o        | Leitura colunar eficiente, √≥timo para analytics, compress√£o forte | N√£o √© leg√≠vel diretamente, sem suporte a updates          |\n",
    "| **ORC**  | ‚ùå N√£o          | ‚úÖ Alta      | ‚úÖ Sim   | ‚ùå N√£o        | Compacta√ß√£o superior, muito usado com Hive                      | Menos compat√≠vel fora do ecossistema Hadoop               |\n",
    "| **Delta** | ‚ùå N√£o         | ‚úÖ Alta      | ‚úÖ Sim   | ‚úÖ Sim        | Hist√≥rico de vers√µes, suporta UPDATE/DELETE/MERGE, confi√°vel    | Requer engine compat√≠vel (Spark, Databricks, EMR)         |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Quando usar cada um?\n",
    "\n",
    "- **CSV**: Ideal para ingest√£o inicial, exporta√ß√£o manual, integra√ß√£o com Excel e fontes legadas\n",
    "- **Parquet**: Melhor escolha para Data Lake anal√≠tico (consultas com filtros e agrega√ß√µes)\n",
    "- **ORC**: Ideal em ecossistemas Hadoop com Hive ou Impala\n",
    "- **Delta**: Formato ideal para arquitetura Lakehouse (Bronze/Silver/Gold), pipelines confi√°veis e versionamento de dados\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Exemplo pr√°tico:\n",
    "\n",
    "- Use **CSV** para uploads manuais e coleta de dados simples.\n",
    "- Converta para **Parquet** assim que poss√≠vel para efici√™ncia.\n",
    "- Evolua para **Delta** se precisar de:\n",
    "  - Atualiza√ß√µes nos dados\n",
    "  - Controle de vers√µes\n",
    "  - Qualidade e governan√ßa com ACID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíß Passo a Passo: Implementando com Data Lake e Lakehouse\n",
    "\n",
    "---\n",
    "\n",
    "### ü™£ 1. Data Lake ‚Äì Armazenamento bruto no Amazon S3\n",
    "\n",
    "No Data Lake, armazenamos os arquivos em formato bruto (CSV, JSON, Parquet) em um bucket no Amazon S3, organizando em camadas por pastas.\n",
    "\n",
    "#### ‚úÖ Exemplo de estrutura de pastas no S3:\n",
    "\n",
    "s3://meu-datalake/raw/clientes/\n",
    "\n",
    "s3://meu-datalake/raw/produtos/\n",
    "\n",
    "s3://meu-datalake/raw/pedidos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚úÖ Exemplo de arquivos:\n",
    "\n",
    "**clientes.csv**\n",
    "```csv\n",
    "id_cliente,nome,idade,cidade\n",
    "1,Carlos Silva,35,S√£o Paulo\n",
    "2,Ana Souza,28,Rio de Janeiro\n",
    "3,Marcos Lima,42,Belo Horizonte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**produtos.json**\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id_produto\": 1001,\n",
    "    \"nome_produto\": \"Vitamina C\",\n",
    "    \"categoria\": \"Suplementos\",\n",
    "    \"preco\": 29.99\n",
    "  },\n",
    "  {\n",
    "    \"id_produto\": 1002,\n",
    "    \"nome_produto\": \"Paracetamol\",\n",
    "    \"categoria\": \"Medicamentos\",\n",
    "    \"preco\": 19.99\n",
    "  },\n",
    "  {\n",
    "    \"id_produto\": 1003,\n",
    "    \"nome_produto\": \"Protetor Solar\",\n",
    "    \"categoria\": \"Beleza\",\n",
    "    \"preco\": 39.99\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pedidos.csv**\n",
    "\n",
    "```csv\n",
    "id_pedido,id_cliente,id_produto,data_pedido,quantidade,valor_total\n",
    "101,1,1001,2025-02-10,2,59.98\n",
    "102,2,1002,2025-02-11,1,19.99\n",
    "103,3,1003,2025-02-12,3,119.97\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Passo a Passo: Configurar Credenciais da AWS Academy\n",
    "\n",
    "Este guia mostra como configurar corretamente as credenciais tempor√°rias fornecidas pela **AWS Academy** (via Learner Lab) para uso com `boto3` e AWS CLI.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Etapa 1 ‚Äì Acessar o AWS Academy Learner Lab\n",
    "\n",
    "1. Acesse: [https://lab.awsacademy.com/](https://lab.awsacademy.com/)\n",
    "2. Fa√ßa login com sua conta AWS Academy\n",
    "3. Inicie o laborat√≥rio (por exemplo, *Learner Lab Environment*)\n",
    "4. Clique em **\"AWS Details\"** ou **\"Show AWS CLI credentials\"**\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Etapa 2 ‚Äì Copiar as credenciais tempor√°rias\n",
    "\n",
    "Voc√™ ver√° 3 valores importantes:\n",
    "\n",
    "```text\n",
    "AWS_ACCESS_KEY_ID=AKIA...\n",
    "AWS_SECRET_ACCESS_KEY=abcd1234...\n",
    "AWS_SESSION_TOKEN=FQoGZXIvYXdzE...\n",
    "``````\n",
    "\n",
    "### ‚ö†Ô∏è Essas credenciais expiram em at√© 4 horas, ent√£o devem ser usadas apenas durante a sess√£o ativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™£ Criando Bucket no S3 e Estrutura para Data Lake (AWS Academy)\n",
    "\n",
    "Este passo a passo mostra como criar um bucket no S3 usando o SDK da AWS (`boto3`) diretamente no notebook, testar a conex√£o e estruturar pastas para um Data Lake.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Pr√©-requisitos\n",
    "\n",
    "Antes de come√ßar, certifique-se de que:\n",
    "\n",
    "- A **biblioteca `boto3`** est√° instalada\n",
    "- Sua conta AWS Academy est√° **configurada com `aws configure`**\n",
    "\n",
    "#### Instalar `boto3` (se necess√°rio)\n",
    "\n",
    "```bash\n",
    "pip install boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ 1. Criar o bucket e testar conex√£o com a AWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# Configura√ß√£o da sess√£o AWS\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=\"ASIA4HYRRT2A467HLD5S\",\n",
    "    aws_secret_access_key=\"tdt6xTT1AV7l4LGnDsoe/NAko+hkHpLg266g1K17\",\n",
    "    aws_session_token=\"IQoJb3JpZ2luX2VjENz//////////wEaCXVzLXdlc3QtMiJHMEUCIQCdThcUsa7HqVHAFDncr5NwJX2d/vSdesZxb2K2K7PGFAIgDOGWz5vcIHPHcDY46JHXSpYwzkLgMbQUZ8zM0UJhlNAqvgIItf//////////ARAAGgw4NDEzMTM1MjUzNzciDAUwGTYS5VS0e49ijiqSAv0XP5n/fkyeqVP8hRNQTNea9owYJB73gCyVCBp46x2HTP/9mftVGy9v0RV6XEZ9RC+QUMV1bCEQYi3+WRoA9cQ6j/hyP3ywFpofYJYXjR6NQAQ/x/MQiRjYwvvABt5rgX2G6EWSGaGvvttj8ZmnEbkrQJZM54er7O29MHdKzo3iulJzviFZFsb+L+3BdoDFDqfHkFOY00Z4oBaUaYls/+SGYWYTe09T0eOApFAi7ZRcR0NxkjKJ9OwNmYh9Nomha04pUU0UU1H6qlTN80oic2EbhE0qUJU1ML8MFO/maixaokYv2q9rti7+xsA7brDxaVitQN9/dTFHOXh3DoRXjTMRXArI4tSWZXvstZG498tvddQwnM6ewgY6nQHIFuwIPGiHK/eOREnoAm4wkH1wQxOQFJg3EUXRbHNgFMPcyZ3f9EUekIbPEzWt7qsrhvgI132KHCj83B6eh8HfJvtJXg7uR84mU6phht+M5GPyFWN5AWWeeEprlSEz0AknVsxBWnwrGN/oB5mz2UqfX0doRSfYOPHnP+z8JAveZkDwZ9UvrQNJojrL1dEaXsOVyoOcBq5kXd6DX+ng\"\n",
    ")\n",
    "\n",
    "# Cria cliente do S3\n",
    "s3 = session.client(\"s3\")\n",
    "print(s3.list_buckets())\n",
    "\n",
    "\n",
    "# Nome do bucket (deve ser √∫nico globalmente)\n",
    "bucket_name = \"aula-data-lake\"\n",
    "\n",
    "\n",
    "# Testa a conex√£o com a conta AWS\n",
    "try:\n",
    "    sts = boto3.client(\"sts\")\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"‚úÖ Conectado √† conta AWS: {identity['Account']} com ARN: {identity['Arn']}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Falha ao conectar na conta AWS:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenta criar o bucket\n",
    "try:\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': boto3.session.Session().region_name}\n",
    "    )\n",
    "    print(f\"‚úÖ Bucket '{bucket_name}' criado com sucesso.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f\"‚ÑπÔ∏è Bucket '{bucket_name}' j√° existe e pertence √† sua conta.\")\n",
    "    else:\n",
    "        print(\"‚ùå Erro ao criar bucket:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ 2. Criar pastas (prefixos) para o Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixos simulando pastas\n",
    "pastas = [\n",
    "    \"raw/clientes/.keep\",\n",
    "    \"raw/produtos/.keep\",\n",
    "    \"raw/pedidos/.keep\",\n",
    "    \"bronze/clientes/.keep\",\n",
    "    \"bronze/produtos/.keep\",\n",
    "    \"bronze/pedidos/.keep\",\n",
    "    \"silver/pedidos_enriquecidos/.keep\",\n",
    "    \"gold/vendas_por_cidade/.keep\"\n",
    "]\n",
    "\n",
    "# Enviar arquivos vazios para criar a estrutura\n",
    "for pasta in pastas:\n",
    "    s3.put_object(Bucket=bucket_name, Key=pasta, Body=b\"\")\n",
    "    print(f\"üìÇ Criado: s3://{bucket_name}/{pasta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ 3. Verificar a estrutura criada no bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=\"\", Delimiter=\"/\")\n",
    "\n",
    "print(\"üìÅ Estrutura inicial do bucket:\")\n",
    "for content in response.get(\"Contents\", []):\n",
    "    print(\" -\", content['Key'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Leitura com PySpark (modo Data Lake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LakehouseExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Leitura com PySpark (modo Data Lake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = spark.read.csv(\"s3://meu-datalake/raw/clientes/\", header=True, inferSchema=True)\n",
    "df_produtos = spark.read.json(\"s3://meu-datalake/raw/produtos/\")\n",
    "df_pedidos = spark.read.csv(\"s3://meu-datalake/raw/pedidos/\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè† Modelo Lakehouse com Camadas Bronze, Silver e Gold\n",
    "\n",
    "No modelo **Lakehouse**, os dados passam por camadas **Bronze**, **Silver** e **Gold**, com uso do formato **Delta** para garantir **controle de vers√£o** e **transa√ß√µes ACID** (Atomicidade, Consist√™ncia, Isolamento e Durabilidade).\n",
    "\n",
    "Essa estrutura permite combinar a flexibilidade do Data Lake com a governan√ßa e a confiabilidade de um Data Warehouse.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Estrutura no S3 com Delta Lake\n",
    "\n",
    "```text\n",
    "s3://meu-lakehouse/bronze/clientes/\n",
    "s3://meu-lakehouse/bronze/produtos/\n",
    "s3://meu-lakehouse/bronze/pedidos/\n",
    "\n",
    "s3://meu-lakehouse/silver/clientes_limpos/\n",
    "s3://meu-lakehouse/silver/pedidos_enriquecidos/\n",
    "\n",
    "s3://meu-lakehouse/gold/vendas_por_cidade/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè† Exemplo de Lakehouse na AWS ‚Äì Passo a Passo\n",
    "\n",
    "Este exemplo mostra como implementar um **Lakehouse** com arquitetura de camadas (Bronze, Silver, Gold), utilizando servi√ßos da **AWS** e o formato **Delta Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Ferramentas Utilizadas\n",
    "\n",
    "| Componente         | Servi√ßo AWS                        |\n",
    "|--------------------|-------------------------------------|\n",
    "| Armazenamento      | Amazon S3                           |\n",
    "| Processamento      | AWS Glue ou Amazon EMR (com Spark)  |\n",
    "| Consultas SQL      | Amazon Athena (com Apache Iceberg) ou Spark SQL |\n",
    "| Dashboard          | Amazon QuickSight (ou Streamlit)    |\n",
    "| Formato dos dados  | Delta Lake (.delta)                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Estrutura de Diret√≥rios no S3 ‚Äì Lakehouse\n",
    "\n",
    "Abaixo est√° a organiza√ß√£o recomendada do bucket no Amazon S3 para um Lakehouse com camadas **Bronze**, **Silver** e **Gold**:\n",
    "\n",
    "```text\n",
    "s3://meu-lakehouse/\n",
    "‚îú‚îÄ‚îÄ bronze/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ clientes/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ produtos/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ pedidos/\n",
    "‚îú‚îÄ‚îÄ silver/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ clientes_limpos/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ pedidos_enriquecidos/\n",
    "‚îî‚îÄ‚îÄ gold/\n",
    "    ‚îî‚îÄ‚îÄ vendas_por_cidade/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Etapa 1: Ingest√£o na Camada Bronze\n",
    "\n",
    "Armazene os dados brutos no S3, diretamente de arquivos CSV, JSON ou APIs externas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo com PySpark\n",
    "df_clientes = spark.read.csv(\"s3://origem-dados/clientes.csv\", header=True, inferSchema=True)\n",
    "df_clientes.write.format(\"delta\").mode(\"overwrite\").save(\"s3://meu-lakehouse/bronze/clientes\")\n",
    "\n",
    "df_produtos = spark.read.json(\"s3://origem-dados/produtos.json\")\n",
    "df_produtos.write.format(\"delta\").mode(\"overwrite\").save(\"s3://meu-lakehouse/bronze/produtos\")\n",
    "\n",
    "df_pedidos = spark.read.json(\"s3://origem-dados/pedidos.csv\", header=True, inferSchema=True)\n",
    "df_pedidos.write.format(\"delta\").mode(\"overwrite\").save(\"s3://meu-lakehouse/bronze/pedidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Etapa 2: Transforma√ß√£o na Camada Silver\n",
    "\n",
    "Nesta camada, os dados s√£o **filtrados, deduplicados e integrados**.  \n",
    "√â onde aplicamos as **regras de neg√≥cio** e os **relacionamentos entre dimens√µes**, como:\n",
    "\n",
    "- Clientes\n",
    "- Produtos\n",
    "- Pedidos\n",
    "\n",
    "O objetivo da Silver Layer √© transformar os dados brutos da Bronze em dados estruturados e prontos para an√°lises mais confi√°veis nas camadas anal√≠ticas (Gold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bronze_clientes = spark.read.format(\"delta\").load(\"s3://meu-lakehouse/bronze/clientes\")\n",
    "df_bronze_produtos = spark.read.format(\"delta\").load(\"s3://meu-lakehouse/bronze/produtos\")\n",
    "df_bronze_pedidos = spark.read.format(\"delta\").load(\"s3://meu-lakehouse/bronze/pedidos\")\n",
    "\n",
    "df_enriquecido = df_bronze_pedidos \\\n",
    "    .join(df_bronze_clientes, \"id_cliente\") \\\n",
    "    .join(df_bronze_produtos, \"id_produto\")\n",
    "\n",
    "df_enriquecido.write.format(\"delta\").mode(\"overwrite\").save(\"s3://meu-lakehouse/silver/pedidos_enriquecidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Etapa 3: Agrega√ß√£o na Camada Gold\n",
    "\n",
    "Na camada **Gold**, criamos **datasets agregados e otimizados** para consumo anal√≠tico.\n",
    "\n",
    "Essa camada √© voltada para ferramentas de BI, dashboards e relat√≥rios executivos, com foco em:\n",
    "\n",
    "- Performance de leitura\n",
    "- Agrega√ß√µes pr√©-processadas\n",
    "- Estrutura de f√°cil entendimento por usu√°rios de neg√≥cio\n",
    "\n",
    "Exemplos de uso:\n",
    "\n",
    "- Vendas por cidade\n",
    "- Total de pedidos por cliente\n",
    "- Faturamento por categoria de produto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "\n",
    "df_gold = df_enriquecido.groupBy(\"cidade\").agg(\n",
    "    sum(\"valor_total\").alias(\"vendas_totais\"),\n",
    "    count(\"id_pedido\").alias(\"qtd_pedidos\")\n",
    ")\n",
    "\n",
    "df_gold.write.format(\"delta\").mode(\"overwrite\").save(\"s3://meu-lakehouse/gold/vendas_por_cidade\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Etapa 4: Visualiza√ß√£o com QuickSight ou Streamlit\n",
    "\n",
    "---\n",
    "\n",
    "### üÖ∞Ô∏è QuickSight\n",
    "\n",
    "- Configure um **Data Catalog** usando o **AWS Glue**.\n",
    "- Crie uma **tabela externa** apontando para os arquivos `.delta`.\n",
    "- Importe a tabela `vendas_por_cidade` no QuickSight.\n",
    "- Crie gr√°ficos interativos e dashboards com base nesses dados.\n",
    "\n",
    "---\n",
    "\n",
    "### üÖ±Ô∏è Streamlit (exemplo simples com Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"s3://meu-lakehouse/gold/vendas_por_cidade\").toPandas()\n",
    "fig = px.bar(df, x=\"cidade\", y=\"vendas_totais\", title=\"Vendas por Cidade\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclus√£o\n",
    "\n",
    "A arquitetura **Lakehouse** com camadas **Bronze ‚Üí Silver ‚Üí Gold** permite:\n",
    "\n",
    "- üìÇ Organizar os dados por **n√≠vel de maturidade**\n",
    "- üîÑ Realizar **transforma√ß√µes estruturadas e seguras**\n",
    "- üìú Suportar **versionamento, auditoria e governan√ßa** com Delta Lake\n",
    "- üß† Combinar o melhor do **Data Lake** (flexibilidade) com o **Data Warehouse** (consist√™ncia e controle)\n",
    "\n",
    "Essa abordagem √© ideal para pipelines modernos, escal√°veis e confi√°veis, permitindo an√°lises em tempo real e integra√ß√£o com m√∫ltiplas ferramentas de BI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
