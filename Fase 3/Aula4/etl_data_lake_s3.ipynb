{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Processamento de Dados no Data Lake com Pandas e S3\n","\n","Este notebook apresenta o processo de ingestão, limpeza, transformação e enriquecimento de dados em um ambiente de Data Lake utilizando Pandas, com leitura e escrita diretamente no Amazon S3.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime"]},{"cell_type":"markdown","metadata":{},"source":["## Definição dos caminhos S3\n","Definimos os prefixos de cada camada (raw, silver e gold) para acesso aos arquivos armazenados no bucket do S3."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["RAW_PREFIX = 's3://aula-data-lake/raw/'\n","SILVER_PREFIX = 's3://aula-data-lake/silver/'\n","GOLD_PREFIX = 's3://aula-data-lake/gold/'"]},{"cell_type":"markdown","metadata":{},"source":["## Silver Layer: Leitura e Limpeza dos Dados\n","Nesta etapa, os dados são lidos da camada **raw**, as colunas são padronizadas e os dados são limpos para remoção de duplicidades e nulos críticos."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["clientes = pd.read_csv(RAW_PREFIX + 'clientes.csv')\n","produtos = pd.read_csv(RAW_PREFIX + 'produtos.csv')\n","tipos_produto = pd.read_csv(RAW_PREFIX + 'tipos_produto.csv')\n","pedidos = pd.read_csv(RAW_PREFIX + 'pedidos.csv')\n","itens_pedido = pd.read_csv(RAW_PREFIX + 'itens_pedido.csv')\n","\n","# Padronizar colunas para lower case\n","for df in [clientes, produtos, tipos_produto, pedidos, itens_pedido]:\n","    df.columns = [col.lower() for col in df.columns]\n","\n","# Limpeza básica\n","clientes = clientes.drop_duplicates().dropna(subset=['id_cliente'])\n","produtos = produtos.drop_duplicates().dropna(subset=['id_produto'])\n","tipos_produto = tipos_produto.drop_duplicates().dropna(subset=['id_tipo'])\n","pedidos = pedidos.drop_duplicates().dropna(subset=['id_pedido', 'id_cliente'])\n","itens_pedido = itens_pedido.drop_duplicates().dropna(subset=['id_item', 'id_pedido', 'id_produto'])\n","\n","# Padronização de datas\n","if 'data_pedido' in pedidos.columns:\n","    pedidos['data_pedido'] = pd.to_datetime(pedidos['data_pedido'], errors='coerce')\n","\n","# Salvar camada Silver em Parquet no S3\n","clientes.to_parquet(SILVER_PREFIX + 'clientes/', index=False)\n","produtos.to_parquet(SILVER_PREFIX + 'produtos/', index=False)\n","tipos_produto.to_parquet(SILVER_PREFIX + 'tipos_produto/', index=False)\n","pedidos.to_parquet(SILVER_PREFIX + 'pedidos/', index=False)\n","itens_pedido.to_parquet(SILVER_PREFIX + 'itens_pedido/', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Gold Layer: Enriquecimento e Agregações\n","Criação de fato de vendas e agregações por cliente e tipo de produto, com persistência dos dados em formato particionado por `anomesdia`."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processamento das camadas Silver e Gold no S3 particionado por anomesdia finalizado com sucesso!\n"]}],"source":["gold_vendas = itens_pedido.merge(pedidos, on='id_pedido') \\\n","    .merge(produtos, on='id_produto') \\\n","    .merge(tipos_produto, on='id_tipo') \\\n","    .merge(clientes, on='id_cliente')\n","\n","# Valor total do item\n","if 'quantidade' in gold_vendas.columns and 'preco_unitario' in gold_vendas.columns:\n","    gold_vendas['valor_total_item'] = gold_vendas['quantidade'] * gold_vendas['preco_unitario']\n","\n","# Criação do campo anomesdia\n","if 'data_pedido' in gold_vendas.columns:\n","    gold_vendas['anomesdia'] = gold_vendas['data_pedido'].dt.strftime('%Y%m%d')\n","\n","# Resumo por cliente\n","gold_vendas_por_cliente = gold_vendas.groupby(['id_cliente', 'nome']) \\\n","    .agg({'valor_total_item': 'sum', 'id_pedido': 'nunique'}) \\\n","    .rename(columns={'valor_total_item': 'valor_total_comprado', 'id_pedido': 'num_pedidos'}) \\\n","    .reset_index()\n","\n","# Resumo por tipo de produto\n","gold_vendas_por_tipo = gold_vendas.groupby(['id_tipo', 'nome_tipo']) \\\n","    .agg({'valor_total_item': 'sum', 'quantidade': 'sum'}) \\\n","    .rename(columns={'valor_total_item': 'total_vendido', 'quantidade': 'quantidade_total'}) \\\n","    .reset_index()\n","\n","# Salvar camada Gold no S3, particionando por anomesdia\n","gold_vendas.to_parquet(GOLD_PREFIX + 'fato_vendas/', partition_cols=['anomesdia'], index=False)\n","gold_vendas_por_cliente.to_parquet(GOLD_PREFIX + 'vendas_por_cliente/', index=False)\n","gold_vendas_por_tipo.to_parquet(GOLD_PREFIX + 'vendas_por_tipo/', index=False)\n","\n","print(\"Processamento das camadas Silver e Gold no S3 particionado por anomesdia finalizado com sucesso!\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
